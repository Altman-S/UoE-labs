{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introductory applied machine learning (INFR10069) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5: Neural Networks\n",
    "\n",
    "In this lab we will perform classification on handwritten digits. We will use the UCI Pen-Based Recognition of Handwritten Digits Data Set, which you can read more about below.\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import ParameterGrid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Dataset exploration\n",
    "\n",
    "In this part we will familiarize ourselves with the dataset. Run the below cell to load the digits dataset directly from scikit.\n",
    "\n",
    "The digits dataset contains images of handwritten digits and their corresponding class. We will train classifiers to predict which digit is in an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The digits dataset\n",
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'feature_names', 'target_names', 'images', 'DESCR'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.1 ==========\n",
    "\n",
    "Create a pandas dataframe from the digits dataset. You might find it useful to inspect the result of the `.keys()` function to see what's in the the `digits` dataset.\n",
    "\n",
    "1. How many features do the images have?\n",
    "2. What is the difference between the `data` and `images` fields?\n",
    "\n",
    "Label the features `X1` - `XN`, where `N` is the number of features. Label the target `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>X10</th>\n",
       "      <th>...</th>\n",
       "      <th>X56</th>\n",
       "      <th>X57</th>\n",
       "      <th>X58</th>\n",
       "      <th>X59</th>\n",
       "      <th>X60</th>\n",
       "      <th>X61</th>\n",
       "      <th>X62</th>\n",
       "      <th>X63</th>\n",
       "      <th>X64</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1792</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1793</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1794</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1795</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1796</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1797 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       X1   X2    X3    X4    X5    X6   X7   X8   X9  X10  ...  X56  X57  \\\n",
       "0     0.0  0.0   5.0  13.0   9.0   1.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "1     0.0  0.0   0.0  12.0  13.0   5.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "2     0.0  0.0   0.0   4.0  15.0  12.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "3     0.0  0.0   7.0  15.0  13.0   1.0  0.0  0.0  0.0  8.0  ...  0.0  0.0   \n",
       "4     0.0  0.0   0.0   1.0  11.0   0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "...   ...  ...   ...   ...   ...   ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "1792  0.0  0.0   4.0  10.0  13.0   6.0  0.0  0.0  0.0  1.0  ...  0.0  0.0   \n",
       "1793  0.0  0.0   6.0  16.0  13.0  11.0  1.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "1794  0.0  0.0   1.0  11.0  15.0   1.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "1795  0.0  0.0   2.0  10.0   7.0   0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "1796  0.0  0.0  10.0  14.0   8.0   1.0  0.0  0.0  0.0  2.0  ...  0.0  0.0   \n",
       "\n",
       "      X58  X59   X60   X61   X62  X63  X64  y  \n",
       "0     0.0  6.0  13.0  10.0   0.0  0.0  0.0  0  \n",
       "1     0.0  0.0  11.0  16.0  10.0  0.0  0.0  1  \n",
       "2     0.0  0.0   3.0  11.0  16.0  9.0  0.0  2  \n",
       "3     0.0  7.0  13.0  13.0   9.0  0.0  0.0  3  \n",
       "4     0.0  0.0   2.0  16.0   4.0  0.0  0.0  4  \n",
       "...   ...  ...   ...   ...   ...  ...  ... ..  \n",
       "1792  0.0  2.0  14.0  15.0   9.0  0.0  0.0  9  \n",
       "1793  0.0  6.0  16.0  14.0   6.0  0.0  0.0  0  \n",
       "1794  0.0  2.0   9.0  13.0   6.0  0.0  0.0  8  \n",
       "1795  0.0  5.0  12.0  16.0  12.0  0.0  0.0  9  \n",
       "1796  1.0  8.0  12.0  14.0  12.0  1.0  0.0  8  \n",
       "\n",
       "[1797 rows x 65 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code goes here\n",
    "features = []\n",
    "for i in range(len(digits['feature_names'])):\n",
    "    a = 'X' + str(i + 1)\n",
    "    features.extend([a])\n",
    "\n",
    "df = pd.DataFrame(digits['data'], columns=features)\n",
    "df['y'] = digits['target']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.2 ==========\n",
    "\n",
    "Use the `.describe()` function on the data frame. What does the mean of each feature tell us about the images? Do all the features carry the same amount of information? \n",
    "\n",
    "What do you expect the mean of `y` to be if the dataset is balanced? If so, is this enough information to justify that the dataset is balanced or do we need something more?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>X10</th>\n",
       "      <th>...</th>\n",
       "      <th>X56</th>\n",
       "      <th>X57</th>\n",
       "      <th>X58</th>\n",
       "      <th>X59</th>\n",
       "      <th>X60</th>\n",
       "      <th>X61</th>\n",
       "      <th>X62</th>\n",
       "      <th>X63</th>\n",
       "      <th>X64</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1797.0</td>\n",
       "      <td>1797.000000</td>\n",
       "      <td>1797.000000</td>\n",
       "      <td>1797.000000</td>\n",
       "      <td>1797.000000</td>\n",
       "      <td>1797.000000</td>\n",
       "      <td>1797.000000</td>\n",
       "      <td>1797.000000</td>\n",
       "      <td>1797.000000</td>\n",
       "      <td>1797.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1797.000000</td>\n",
       "      <td>1797.000000</td>\n",
       "      <td>1797.000000</td>\n",
       "      <td>1797.000000</td>\n",
       "      <td>1797.000000</td>\n",
       "      <td>1797.000000</td>\n",
       "      <td>1797.000000</td>\n",
       "      <td>1797.000000</td>\n",
       "      <td>1797.000000</td>\n",
       "      <td>1797.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.303840</td>\n",
       "      <td>5.204786</td>\n",
       "      <td>11.835838</td>\n",
       "      <td>11.848080</td>\n",
       "      <td>5.781859</td>\n",
       "      <td>1.362270</td>\n",
       "      <td>0.129661</td>\n",
       "      <td>0.005565</td>\n",
       "      <td>1.993879</td>\n",
       "      <td>...</td>\n",
       "      <td>0.206455</td>\n",
       "      <td>0.000556</td>\n",
       "      <td>0.279354</td>\n",
       "      <td>5.557596</td>\n",
       "      <td>12.089037</td>\n",
       "      <td>11.809126</td>\n",
       "      <td>6.764051</td>\n",
       "      <td>2.067891</td>\n",
       "      <td>0.364496</td>\n",
       "      <td>4.490818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.907192</td>\n",
       "      <td>4.754826</td>\n",
       "      <td>4.248842</td>\n",
       "      <td>4.287388</td>\n",
       "      <td>5.666418</td>\n",
       "      <td>3.325775</td>\n",
       "      <td>1.037383</td>\n",
       "      <td>0.094222</td>\n",
       "      <td>3.196160</td>\n",
       "      <td>...</td>\n",
       "      <td>0.984401</td>\n",
       "      <td>0.023590</td>\n",
       "      <td>0.934302</td>\n",
       "      <td>5.103019</td>\n",
       "      <td>4.374694</td>\n",
       "      <td>4.933947</td>\n",
       "      <td>5.900623</td>\n",
       "      <td>4.090548</td>\n",
       "      <td>1.860122</td>\n",
       "      <td>2.865304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.0</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           X1           X2           X3           X4           X5  \\\n",
       "count  1797.0  1797.000000  1797.000000  1797.000000  1797.000000   \n",
       "mean      0.0     0.303840     5.204786    11.835838    11.848080   \n",
       "std       0.0     0.907192     4.754826     4.248842     4.287388   \n",
       "min       0.0     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.0     0.000000     1.000000    10.000000    10.000000   \n",
       "50%       0.0     0.000000     4.000000    13.000000    13.000000   \n",
       "75%       0.0     0.000000     9.000000    15.000000    15.000000   \n",
       "max       0.0     8.000000    16.000000    16.000000    16.000000   \n",
       "\n",
       "                X6           X7           X8           X9          X10  ...  \\\n",
       "count  1797.000000  1797.000000  1797.000000  1797.000000  1797.000000  ...   \n",
       "mean      5.781859     1.362270     0.129661     0.005565     1.993879  ...   \n",
       "std       5.666418     3.325775     1.037383     0.094222     3.196160  ...   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "50%       4.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "75%      11.000000     0.000000     0.000000     0.000000     3.000000  ...   \n",
       "max      16.000000    16.000000    15.000000     2.000000    16.000000  ...   \n",
       "\n",
       "               X56          X57          X58          X59          X60  \\\n",
       "count  1797.000000  1797.000000  1797.000000  1797.000000  1797.000000   \n",
       "mean      0.206455     0.000556     0.279354     5.557596    12.089037   \n",
       "std       0.984401     0.023590     0.934302     5.103019     4.374694   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     1.000000    11.000000   \n",
       "50%       0.000000     0.000000     0.000000     4.000000    13.000000   \n",
       "75%       0.000000     0.000000     0.000000    10.000000    16.000000   \n",
       "max      13.000000     1.000000     9.000000    16.000000    16.000000   \n",
       "\n",
       "               X61          X62          X63          X64            y  \n",
       "count  1797.000000  1797.000000  1797.000000  1797.000000  1797.000000  \n",
       "mean     11.809126     6.764051     2.067891     0.364496     4.490818  \n",
       "std       4.933947     5.900623     4.090548     1.860122     2.865304  \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "25%      10.000000     0.000000     0.000000     0.000000     2.000000  \n",
       "50%      14.000000     6.000000     0.000000     0.000000     4.000000  \n",
       "75%      16.000000    12.000000     2.000000     0.000000     7.000000  \n",
       "max      16.000000    16.000000    16.000000    16.000000     9.000000  \n",
       "\n",
       "[8 rows x 65 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code goes here\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.3 ==========\n",
    "\n",
    "Use `plt.imshow()` to visualize the images in the below figure. The function takes a 2D array as input.\n",
    "\n",
    "Plot the images with index 1, 101, 50, and 750."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPUAAAD4CAYAAAA0L6C7AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAAKtUlEQVR4nO3d/2td9R3H8ddr0bI5nYW1DGlKkx+kIAMTuRSkQ1zFUadoftgPLShUBv4ypWUD0f3U/gOS/TAEqRrBTtmqVhGnE1Q2YXOmNd1so6MrGU3RtWUEv/ywUn3vh5xClXQ5995zzj159/mAYJJ7yed9qc+ee09uz8cRIQB5fGPQAwCoFlEDyRA1kAxRA8kQNZDMZXX80DVr1sTIyEgdP/qSsrCw0NhaJ06caGytVatWNbbW6OhoY2tJzT22ubk5nTlzxkvdVkvUIyMjmp6eruNHX1JefPHFxtbauXNnY2s1+Rf+1NRUY2tJzT22Tqdz0dt4+g0kQ9RAMkQNJEPUQDJEDSRD1EAyRA0kQ9RAMkQNJFMqattbbX9o+5jth+oeCkDvlo3a9pCkX0u6TdJ1krbbvq7uwQD0psyRepOkYxFxPCLOSnpW0l31jgWgV2WiXifpwn/CM1987yts32d72vb06dOnq5oPQJcqO1EWEY9FRCciOmvXrq3qxwLoUpmoT0paf8HXw8X3ALRQmajflXSt7VHbqyRtk/RSvWMB6NWyF0mIiHO275f0mqQhSU9ExJHaJwPQk1JXPomIVyS9UvMsACrAO8qAZIgaSIaogWSIGkiGqIFkiBpIhqiBZGrZoSOrycnJRtfbvXt3Y2vt2rWrsbWa3DVjbm6usbWkZncfuRiO1EAyRA0kQ9RAMkQNJEPUQDJEDSRD1EAyRA0kQ9RAMkQNJFNmh44nbJ+y/X4TAwHoT5kj9ZSkrTXPAaAiy0YdEX+U9J8GZgFQgcpeU7PtDtAObLsDJMPZbyAZogaSKfMrrWck/VnSRtvztn9a/1gAelVmL63tTQwCoBo8/QaSIWogGaIGkiFqIBmiBpIhaiAZogaSYdudLqxevbrR9WZmZhpba2FhobG1Dhw40NhaY2Njja3VFhypgWSIGkiGqIFkiBpIhqiBZIgaSIaogWSIGkiGqIFkiBpIpsw1ytbbftP2UdtHbO9sYjAAvSnz3u9zkn4REYdsXyXpoO3XI+JozbMB6EGZbXc+iohDxeefSpqVtK7uwQD0pqvX1LZHJI1LemeJ29h2B2iB0lHbvlLSc5J2RcQnX7+dbXeAdigVte3LtRj0voh4vt6RAPSjzNlvS3pc0mxEPFL/SAD6UeZIvVnSPZK22J4pPn5c81wAelRm2523JbmBWQBUgHeUAckQNZAMUQPJEDWQDFEDyRA1kAxRA8kQNZAMe2l1YceOHY2u1+T+VhMTE42tNTk52dhaTe9/1gYcqYFkiBpIhqiBZIgaSIaogWSIGkiGqIFkiBpIhqiBZMpcePCbtv9q+3Cx7c6eJgYD0JsybxP9r6QtEfFZcangt23/PiL+UvNsAHpQ5sKDIemz4svLi4+ocygAvSt7Mf8h2zOSTkl6PSLYdgdoqVJRR8QXETEmaVjSJtvfX+I+bLsDtEBXZ78jYkHSm5K21jINgL6VOfu91vbq4vNvSbpV0gc1zwWgR2XOfl8j6SnbQ1r8S+C3EfFyvWMB6FWZs99/0+Ke1ABWAN5RBiRD1EAyRA0kQ9RAMkQNJEPUQDJEDSRD1EAybLvThSa3wZGa3ebn5ptvTrnWpYgjNZAMUQPJEDWQDFEDyRA1kAxRA8kQNZAMUQPJEDWQDFEDyZSOurig/3u2uegg0GLdHKl3SpqtaxAA1Si77c6wpNsl7a13HAD9KnuknpT0oKQvL3YH9tIC2qHMDh13SDoVEQf/3/3YSwtohzJH6s2S7rQ9J+lZSVtsP13rVAB6tmzUEfFwRAxHxIikbZLeiIi7a58MQE/4PTWQTFeXM4qItyS9VcskACrBkRpIhqiBZIgaSIaogWSIGkiGqIFkiBpIZsVvu7N79+7G1tqzZ09ja0nS9ddf39haBw4caGwt1IsjNZAMUQPJEDWQDFEDyRA1kAxRA8kQNZAMUQPJEDWQDFEDyZR6m2hxJdFPJX0h6VxEdOocCkDvunnv9w8j4kxtkwCoBE+/gWTKRh2S/mD7oO37lroD2+4A7VA26h9ExA2SbpP0M9s3ff0ObLsDtEOpqCPiZPHfU5JekLSpzqEA9K7MBnnftn3V+c8l/UjS+3UPBqA3Zc5+f0/SC7bP3/83EfFqrVMB6NmyUUfEcUnNXVcHQF/4lRaQDFEDyRA1kAxRA8kQNZAMUQPJEDWQzIrfdmd8fLyxtTZs2NDYWpJ0+PDhxtaamJhobK3JycnG1hoZGWlsrbbgSA0kQ9RAMkQNJEPUQDJEDSRD1EAyRA0kQ9RAMkQNJEPUQDKlora92vZ+2x/YnrV9Y92DAehN2fd+/0rSqxHxE9urJF1R40wA+rBs1LavlnSTpB2SFBFnJZ2tdywAvSrz9HtU0mlJT9p+z/be4vrfX8G2O0A7lIn6Mkk3SHo0IsYlfS7poa/fiW13gHYoE/W8pPmIeKf4er8WIwfQQstGHREfSzphe2PxrVskHa11KgA9K3v2+wFJ+4oz38cl3VvfSAD6USrqiJiR1Kl3FABV4B1lQDJEDSRD1EAyRA0kQ9RAMkQNJEPUQDJEDSTjiKj8h3Y6nZienq78515qpqamUq61sLDQ2FpNPi5JGhsba2SdTqej6elpL3UbR2ogGaIGkiFqIBmiBpIhaiAZogaSIWogGaIGkiFqIJllo7a90fbMBR+f2N7VwGwAerDsNcoi4kNJY5Jke0jSSUkv1DsWgF51+/T7Fkn/jIh/1TEMgP51G/U2Sc8sdQPb7gDtUDrq4prfd0r63VK3s+0O0A7dHKlvk3QoIv5d1zAA+tdN1Nt1kafeANqjVNTF1rW3Snq+3nEA9KvstjufS/puzbMAqADvKAOSIWogGaIGkiFqIBmiBpIhaiAZogaSIWogmVq23bF9WlK3/zxzjaQzlQ/TDlkfG49rcDZExJL/cqqWqHthezoiOoOeow5ZHxuPq514+g0kQ9RAMm2K+rFBD1CjrI+Nx9VCrXlNDaAabTpSA6gAUQPJtCJq21ttf2j7mO2HBj1PFWyvt/2m7aO2j9jeOeiZqmR7yPZ7tl8e9CxVsr3a9n7bH9ietX3joGfq1sBfUxcbBPxDi5dLmpf0rqTtEXF0oIP1yfY1kq6JiEO2r5J0UNLESn9c59n+uaSOpO9ExB2Dnqcqtp+S9KeI2FtcQfeKiFgY8FhdacORepOkYxFxPCLOSnpW0l0DnqlvEfFRRBwqPv9U0qykdYOdqhq2hyXdLmnvoGepku2rJd0k6XFJioizKy1oqR1Rr5N04oKv55Xkf/7zbI9IGpf0zoBHqcqkpAclfTngOao2Kum0pCeLlxZ7i4turihtiDo121dKek7Sroj4ZNDz9Mv2HZJORcTBQc9Sg8sk3SDp0YgYl/S5pBV3jqcNUZ+UtP6Cr4eL7614ti/XYtD7IiLL5ZU3S7rT9pwWXyptsf30YEeqzLyk+Yg4/4xqvxYjX1HaEPW7kq61PVqcmNgm6aUBz9Q329bia7PZiHhk0PNUJSIejojhiBjR4p/VGxFx94DHqkREfCzphO2NxbdukbTiTmyWuu53nSLinO37Jb0maUjSExFxZMBjVWGzpHsk/d32TPG9X0bEK4MbCSU8IGlfcYA5LuneAc/TtYH/SgtAtdrw9BtAhYgaSIaogWSIGkiGqIFkiBpIhqiBZP4HJzSz1UY/l4YAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# your code goes here\n",
    "a = list(df.iloc[1])[:-1]\n",
    "pic1 = np.reshape(a, newshape=(8, 8))\n",
    "plt.imshow(pic1, cmap='gray_r')\n",
    "plt.show()\n",
    "a101 = list(df.iloc[101])[:-1]\n",
    "pic101 = np.reshape(a101, newshape=(8, 8))\n",
    "plt.imshow(pic101, cmap='gray_r')\n",
    "plt.show()\n",
    "a50 = list(df.iloc[50])[:-1]\n",
    "pic50 = np.reshape(a50, newshape=(8, 8))\n",
    "plt.imshow(pic50, cmap='gray_r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.4 ==========\n",
    "\n",
    "Use `plt.imshow()` to visualize the `mean` and standard deviation (`std`) of the images in the entire dataset. What do you notice about the corner pixels in the image? What does that tell us about the amount of information they give us?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.5 ==========\n",
    "\n",
    "Crate a `train`, `test` *and* `validation` split from the data. Use a 60/20/20 split for train/test/validation.\n",
    "\n",
    "*Hint*: You can use the inbuilt sklearn function `train_test_split` twice, once on the original dataset and once on the resulting set to create the split.\n",
    "\n",
    "Use `random_seed=42`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (1797, 64)\n",
      "y shape: (1797,)\n"
     ]
    }
   ],
   "source": [
    "# your code goes here\n",
    "\n",
    "# Split the data into the features X (pandas dataframe), and the labels y\n",
    "# generate X   delete 'class' column\n",
    "X = pd.get_dummies(df.drop('y', axis=1))\n",
    "column_names = X.columns\n",
    "# Convert to Numpy Array\n",
    "X = X.values\n",
    "print('X shape: {}'.format(X.shape))\n",
    "\n",
    "# generate y\n",
    "y = df['y'].values # Target vector\n",
    "print('y shape: {}'.format(np.shape(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train, y_train, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1077, 64), (360, 64), (360, 64), (1077,), (360,), (360,)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[dd.shape for dd in [X_train, X_valid, X_test, y_train, y_valid, y_test]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Multiclass Linear Model\n",
    "\n",
    "For our baseline, we will use a [LogisticRegression model](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html), which you used in [Lab 03 - Classification and Evaluation](https://github.com/uoe-iaml/iaml-labs/blob/master/Labs/03%20-%20Classification%20and%20Evaluation.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### ========== Question 2.1 ==========\n",
    "Familiarize yourself with the `multi_class` parameter in the LogisticRegression function to train a one-versus-rest multi-class classifier on the training data.\n",
    "\n",
    "Use `accuracy_score` to report the accuracy on thetest set. Did you expect the performance to be this good or bad? Why or why not?\n",
    "\n",
    "Again, use `random_state=42`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/altman/miniconda3/envs/py3iaml/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/altman/miniconda3/envs/py3iaml/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/altman/miniconda3/envs/py3iaml/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/altman/miniconda3/envs/py3iaml/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/altman/miniconda3/envs/py3iaml/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/altman/miniconda3/envs/py3iaml/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/altman/miniconda3/envs/py3iaml/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/altman/miniconda3/envs/py3iaml/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/altman/miniconda3/envs/py3iaml/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/altman/miniconda3/envs/py3iaml/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(multi_class='ovr', random_state=42)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code goes here\n",
    "lr = LogisticRegression(multi_class='ovr', random_state=42)\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9583333333333334"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3. Classification with Neural Networks\n",
    "\n",
    "In this part, we will use [Scikit's MLPClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html) to train a neural network to classify the handwritten digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 3.1 ==========\n",
    "\n",
    "Initialize an MLPClassifier with one hidden layer consisting of 50 neurons, and set early_stopping=True and fit the neural network to the training data. You can leave the other parameters to the default settings. Is the performance better than the linear classifier?\n",
    "\n",
    "As usual, set `random_state=42`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(early_stopping=True, hidden_layer_sizes=50, random_state=42)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code goes here\n",
    "mlp = MLPClassifier(hidden_layer_sizes=50, early_stopping=True, random_state=42)\n",
    "mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9138888888888889"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 3.2 ==========\n",
    "\n",
    "In this part, we will perform a grid search over a few of the `MLPClassifier` parameters. Namely, we will look at\n",
    "\n",
    "- `hidden_layer_sizes`\n",
    "- `activation`\n",
    "- `alpha`\n",
    "- `momentum`\n",
    "\n",
    "Familiarize yourself with the meaning of the above parameters. If you are curious, here is an in-depth, yet very accessible [article on momentum](https://distill.pub/2017/momentum/).\n",
    "\n",
    "Then initialize a dictionary that has as keys the above parameters and the following ranges for each\n",
    "\n",
    "- `hidden_layer_sizes`: 16, 32, 64, 128, 256\n",
    "- `batch_size`: 64, 128, 256, 512\n",
    "\n",
    "There are other parameters we could potential vary. Optionally, after you have completed the rest of the lab, you can try to vary the following. Make sure you read the documentation and know what the values mean first!\n",
    "- `activation`: `relu` and `logistic`\n",
    "- `alpha`: 1e-3, 1e-4, 1e-5\n",
    "\n",
    "This dictionary will be used in the provided code for 3.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "grid = {'hidden_layer_sizes': [16, 32, 64, 128, 256], 'activation': ['relu', 'logistic'], 'alpha': [1e-3, 1e-4, 1e-5], 'batch_size': [64, 128, 256, 512]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 3.3 ==========\n",
    "\n",
    "In this part, we will use [ParameterGrid](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ParameterGrid.html) to loop over all possible combinations of settings for the grid search.\n",
    "\n",
    "Fill in the missing code to instantiate an `MLPClassifier` with the given parameters and fit it on the data.\n",
    "\n",
    "You can use the `.set_params()` method after you have instantiated the `MLPClassifier`. Set `tol=1e-3`, `solver='adam'`, `random_state=42`, and `max_iter=1000`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (693998937.py, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/hc/nb68sdy94m18z5wflv2b1v2c0000gn/T/ipykernel_5920/693998937.py\"\u001b[0;36m, line \u001b[0;32m12\u001b[0m\n\u001b[0;31m    'val_score': # compute the validation set accuracy score\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "mlp.set_params(tol=1e-3, solver='adam', random_state=42, max_iter=1000)\n",
    "grid_search_results = []\n",
    "\n",
    "for g in ParameterGrid(grid):\n",
    "    # Your code goes here\n",
    "    #   Initialize an MLPClassifier and train it on the *training set*\n",
    "    \n",
    "    grid_search_results.append({\n",
    "        'mlp': mlp,\n",
    "        'params': g,\n",
    "        # Your code goes here\n",
    "        'train_score': mlp.score()\n",
    "        'val_score': # compute the validation set accuracy score\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 3.4 ==========\n",
    "\n",
    "In this part, we will plot the effects of hidden layer sizes on accuracy. We will compute the mean accuracy and a standard deviation for each hidden layer size over other parameter changes. In other words, if `hidden_layer_sizes=50`, we have `4` (or `18`, if you varied `activation` and `alpha`) accuracy scores, one for each of the other parameters.\n",
    "\n",
    "Read the plotting code below and write code that computes summary statistics from the results of the grid search.\n",
    "\n",
    "The summary statistics we will look at are the mean and standard deviation of accuracy scores on the training and validation datasets (`train_score` and `val_score` as stored above in `grid_search_results`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_scores_val, std_scores_val = [], []\n",
    "mean_scores_train, std_scores_train = [], []\n",
    "\n",
    "for ls in grid['hidden_layer_sizes']:\n",
    "    mean_scores_val.append(\n",
    "        np.mean([\n",
    "            x['val_score'] for x in grid_search_results if x['params']['hidden_layer_sizes'] == ls\n",
    "        ])\n",
    "    )\n",
    "    std_scores_val.append(\n",
    "        np.std([\n",
    "            x['val_score'] for x in grid_search_results if x['params']['hidden_layer_sizes'] == ls\n",
    "        ])\n",
    "    )\n",
    "    \n",
    "    # Your code goes here.\n",
    "    #  compute the mean and standard deviation on the train set for\n",
    "    #  all classifiers where hidden_layer_sizes=ls\n",
    "\n",
    "mean_scores_val = np.array(mean_scores_val)\n",
    "std_scores_val = np.array(std_scores_val)\n",
    "\n",
    "mean_scores_train = np.array(mean_scores_train)\n",
    "std_scores_train = np.array(std_scores_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "plt.rcParams['font.size'] = 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 7))\n",
    "\n",
    "plt.plot(grid['hidden_layer_sizes'], mean_scores_train, color=cm[0])\n",
    "plt.scatter(grid['hidden_layer_sizes'], mean_scores_train, label='Train score',  color=cm[0])\n",
    "plt.fill_between(\n",
    "    grid['hidden_layer_sizes'],\n",
    "    mean_scores_train - std_scores_train, mean_scores_train + std_scores_train,\n",
    "    alpha=0.3, color=cm[0]\n",
    ")\n",
    "\n",
    "\n",
    "plt.plot(grid['hidden_layer_sizes'], mean_scores_val, color=cm[1])\n",
    "plt.scatter(grid['hidden_layer_sizes'], mean_scores_val, label='Val. score', color=cm[1])\n",
    "plt.fill_between(\n",
    "    grid['hidden_layer_sizes'],\n",
    "    mean_scores_val - std_scores_val, mean_scores_val + std_scores_val,\n",
    "    alpha=0.2, color=cm[1]\n",
    ")\n",
    "\n",
    "plt.xscale('log')\n",
    "\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid()\n",
    "plt.xlabel('Hidden layer size')\n",
    "plt.ylabel('Accuracy score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 3.4 ==========\n",
    "\n",
    "Finally, pick the best classifier based on the accuracy score on the validation set.\n",
    "\n",
    "You can use python's sorted, [which can additionally use a key function](https://docs.python.org/3/howto/sorting.html#key-functions). \n",
    "\n",
    "Lastly, report the score of the best classifier on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4. Feature Importance\n",
    "\n",
    "In this part, we will randomize a feature and look at its effect on classification. This way we can reason about how important a feature is for classification. If the performance stays the same when the feature is removed (or randomized), then we can reason it has low importance and vice-versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 4.1 ==========\n",
    "\n",
    "Randomize the top-left pixel (index 0) in each image in the test set, then use the `MLPClassifier` with the best settings found above by the grid search. Report the test-set accuracy of the already trained model.\n",
    "\n",
    "To randomize, generate a uniform random number by calling [np.random.uniform](https://numpy.org/doc/stable/reference/random/generated/numpy.random.uniform.html). You need to specify the range -- you can use `np.min` and `np.max` on the original dataset to get the correct values.\n",
    "\n",
    "Note in this case we are not selecting a model, rather looking at performance, so we are not using a validation set at all.\n",
    "\n",
    "*Hint*: You can use numpy's [np.copy](https://numpy.org/doc/stable/reference/generated/numpy.copy.html) to copy the train set so you don't overwrite your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set numpy's seed so that we obtain reproducible results\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did the performance change and by how much? Can you explain the difference?\n",
    "\n",
    "*Hint:* Go back to the start of the lab where we used `.describe()` on the dataframe you created. Look at the values of the top-left pixel in the original array. What do you see?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 4.2 ==========\n",
    "\n",
    "Now randomize all the pixels, *one at a time*. For each pixel, record the ratio of the best accuracy score to the one obtained. Use `.imshow()` to plot a 2D grid of the importances for each pixel.\n",
    "\n",
    "Which pixel is the least/most important? Do you have any intuition why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3iaml] *",
   "language": "python",
   "name": "conda-env-py3iaml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
