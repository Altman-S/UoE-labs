{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dda4c8cf",
   "metadata": {},
   "source": [
    "# Import Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "157ee284",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import math\n",
    "import re\n",
    "import numpy as np\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "from stemming.porter2 import stem\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "# from gensim.test.utils import common_texts\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.corpora.dictionary import Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db25a4b0",
   "metadata": {},
   "source": [
    "# Word-Level Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3500e49f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67937\n",
      "2528\n"
     ]
    }
   ],
   "source": [
    "with open('corpus1.txt', 'r') as f:\n",
    "    corpus1 = f.read()\n",
    "\n",
    "with open('corpus2.txt', 'r') as f:\n",
    "    corpus2 = f.read()\n",
    "\n",
    "corp1 = corpus1.split('\\n\\n')\n",
    "corp2 = corpus2.split('\\n\\n')\n",
    "print(len(corp1))\n",
    "print(len(corp2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73522c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete punctuation\n",
    "# re.sub(pattern, repl, string, count=0, flags=0)\n",
    "# put words in englishST into a list\n",
    "with open('englishST.txt', 'r') as eng:\n",
    "    eng_str = eng.read()\n",
    "eng_list = eng_str.split()\n",
    "    \n",
    "r = \"\"\"[0-9!#$%&'\"()*+,-./:;\\\\\\<=>?@[\\]^_`{|}~\\n]\"\"\"\n",
    "\n",
    "\n",
    "def preprocessing(s):\n",
    "    # delete punctuation\n",
    "    no_punct = re.sub(r, ' ', s)\n",
    "\n",
    "    # transform to lower_case\n",
    "    no_punct = no_punct.lower()\n",
    "\n",
    "    # put string to words list\n",
    "    no_list = no_punct.split()\n",
    "\n",
    "    # delete stopping words(englishST.txt) in no_list\n",
    "    stop_list = []\n",
    "    for i in no_list:\n",
    "        if (i not in eng_list):\n",
    "            stop_list.extend([i])\n",
    "    stop_list\n",
    "\n",
    "    # stemming\n",
    "    norm_list = []\n",
    "    for i in stop_list:\n",
    "        norm_list.append(stem(i))\n",
    "    \n",
    "    return norm_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79b78356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get word_list of corp1 and corp2\n",
    "# every list in corp1 represents a document\n",
    "# every list in corp1 represents a document\n",
    "corp1_list = []\n",
    "corp2_list = []\n",
    "\n",
    "for i in range(len(corp1)):\n",
    "    line = preprocessing(corp1[i])\n",
    "    if (len(line) > 0):\n",
    "        corp1_list.extend([line])\n",
    "\n",
    "for j in range(len(corp2)):\n",
    "    line = preprocessing(corp2[j])\n",
    "    if (len(line) > 0):\n",
    "        corp2_list.extend([line])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ec42a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of documents in corp1: 66420\n",
      "number of documents in corp2: 2458\n"
     ]
    }
   ],
   "source": [
    "print(\"number of documents in corp1: \"+ str(len(corp1_list)))\n",
    "print(\"number of documents in corp2: \" + str(len(corp2_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7a7224a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get corpus dictionary and its key_list\n",
    "\n",
    "# opeartion for corpus1\n",
    "corpus1_list = preprocessing(corpus1)\n",
    "corpus1_counter = Counter(corpus1_list)\n",
    "corpus1_dict = dict(corpus1_counter)\n",
    "# operation for corpus2\n",
    "corpus2_list = preprocessing(corpus2)\n",
    "corpus2_counter = Counter(corpus2_list)\n",
    "corpus2_dict = dict(corpus2_counter)\n",
    "\n",
    "# delete when value < 10\n",
    "list1 = []; list2 = []\n",
    "for key, value in corpus1_dict.items():\n",
    "    if (key in corpus2_dict.keys()):\n",
    "        if (corpus2_dict[key] + value >= 10):\n",
    "            list1.append(key)\n",
    "            list2.append(value)\n",
    "    else:\n",
    "        if (value >= 10):\n",
    "            list1.append(key)\n",
    "            list2.append(value)\n",
    "            \n",
    "corpus1_dict = dict(zip(list1,list2))\n",
    "corpus1_key = list(corpus1_dict.keys())\n",
    "\n",
    "# delete when value < 10\n",
    "list1 = []; list2 = []\n",
    "for key, value in corpus2_dict.items():\n",
    "    if (key in corpus1_dict.keys()):\n",
    "        if (corpus1_dict[key] + value >= 10):\n",
    "            list1.append(key)\n",
    "            list2.append(value)\n",
    "    else:\n",
    "        if (value >= 10):\n",
    "            list1.append(key)\n",
    "            list2.append(value)\n",
    "            \n",
    "corpus2_dict = dict(zip(list1,list2))\n",
    "corpus2_key = list(corpus2_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbfa28c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of corpus1_key: 9036\n",
      "length of corpus2_key: 4658\n",
      "length of special_list: 9039\n"
     ]
    }
   ],
   "source": [
    "# special word list in corpus1 and corpus2\n",
    "\n",
    "special_list = list(set(corpus1_key).union(set(corpus2_key)))\n",
    "print('length of corpus1_key: ' + str(len(corpus1_key)))\n",
    "print('length of corpus2_key: ' + str(len(corpus2_key)))\n",
    "print('length of special_list: ' + str(len(special_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5dd52c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the number of times every word appears in the document in each corpus\n",
    "\n",
    "num_corp1_dict = {}\n",
    "for i in range(len(corpus1_key)):\n",
    "    num_corp1_dict[corpus1_key[i]] = 0\n",
    "    \n",
    "for i in range(len(corp1_list)):\n",
    "    w_list = list(np.unique(corp1_list[i]))\n",
    "    for w in w_list:\n",
    "        if (w in corpus1_key):\n",
    "            num_corp1_dict[w] += 1\n",
    "\n",
    "num_corp2_dict = {}\n",
    "for i in range(len(corpus2_key)):\n",
    "    num_corp2_dict[corpus2_key[i]] = 0\n",
    "    \n",
    "for i in range(len(corp2_list)):\n",
    "    w_list = list(np.unique(corp2_list[i]))\n",
    "    for w in w_list:\n",
    "        if (w in corpus2_key):\n",
    "            num_corp2_dict[w] += 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee54a1e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4658"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(num_corp2_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ca65b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute mutual information\n",
    "mi_list = []\n",
    "X2_list = []\n",
    "\n",
    "def takeSecond(elem):\n",
    "    return elem[1]\n",
    "\n",
    "# calaculate N N0 N1 N11 N10 N01 N00 for corpus1\n",
    "N = len(corp1_list) + len(corp2_list)  # corpus1 + corpus2\n",
    "N1 = len(corp1_list)  # corpus1\n",
    "N0 = len(corp2_list)  # corpus2\n",
    "\n",
    "\n",
    "for w in special_list:\n",
    "    if (w in corpus1_key):  # in corpus1\n",
    "        N11 = num_corp1_dict[w]  \n",
    "    else:\n",
    "        N11 = 0\n",
    "    N01 = N1 - N11\n",
    "    \n",
    "    if (w in corpus2_key):  # in corpus2\n",
    "        N10 = num_corp2_dict[w]    \n",
    "    else:\n",
    "        N10 = 0\n",
    "    N00 = N0 - N10\n",
    "    \n",
    "\n",
    "    if (N11 != 0 and N01 != 0 and N10 != 0 and N00 != 0):\n",
    "        a = (N11 / N) * math.log2((N * N11)/((N10+N11) * N1))\n",
    "        b = (N01 / N) * math.log2((N * N01)/((N00+N01) * N1))\n",
    "        c = (N10 / N) * math.log2((N * N10)/((N10+N11) * N0))\n",
    "        d = (N00 / N) * math.log2((N * N00)/((N00+N01) * N0))\n",
    "    \n",
    "    mi = a + b + c + d\n",
    "    X2 = ((N11+N10+N01+N00)*(N11*N00-N10*N01)**2) / ((N11+N01)*(N11+N10)*(N10+N00)*(N01+N00))\n",
    "     \n",
    "    mi_list.extend([[w, mi]])\n",
    "    X2_list.extend([[w, X2]])\n",
    "    \n",
    "mi_list.sort(key=takeSecond, reverse=True)\n",
    "X2_list.sort(key=takeSecond, reverse=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b9ae2f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68878"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(mi_list)\n",
    "# print(X2_list)\n",
    "a = corp1_list.copy()\n",
    "a.extend(corp2_list)\n",
    "len(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed55b140",
   "metadata": {},
   "source": [
    "# TOPIC-LEVEL COMPARISONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d752a350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a corpus from a list of texts\n",
    "common_texts = corp1_list.copy()\n",
    "common_texts.extend(corp2_list)\n",
    "\n",
    "common_dictionary = Dictionary(common_texts)\n",
    "common_corpus = [common_dictionary.doc2bow(text) for text in common_texts]\n",
    "\n",
    "# Train the model on the corpus\n",
    "lda = LdaModel(common_corpus, num_topics=10, id2word=common_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "782ccc10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.028*\"car\" + 0.026*\"sister\" + 0.017*\"drive\" + 0.012*\"citi\" + 0.010*\"time\" + 0.010*\"uber\" + 0.009*\"airlin\" + 0.009*\"servic\" + 0.007*\"phone\" + 0.007*\"day\"'),\n",
       " (1,\n",
       "  '0.025*\"googl\" + 0.022*\"org\" + 0.018*\"https\" + 0.018*\"http\" + 0.016*\"en\" + 0.016*\"wikipedia\" + 0.015*\"wiki\" + 0.015*\"facebook\" + 0.012*\"user\" + 0.010*\"reddit\"'),\n",
       " (2,\n",
       "  '0.025*\"compani\" + 0.017*\"peopl\" + 0.016*\"job\" + 0.013*\"work\" + 0.013*\"make\" + 0.010*\"good\" + 0.009*\"don\" + 0.008*\"bonus\" + 0.008*\"money\" + 0.007*\"busi\"'),\n",
       " (3,\n",
       "  '0.049*\"http\" + 0.044*\"www\" + 0.027*\"reddit\" + 0.026*\"comment\" + 0.021*\"loan\" + 0.017*\"bailout\" + 0.017*\"messag\" + 0.016*\"bank\" + 0.013*\"diamond\" + 0.012*\"news\"'),\n",
       " (4,\n",
       "  '0.018*\"market\" + 0.015*\"price\" + 0.011*\"cost\" + 0.011*\"busi\" + 0.008*\"product\" + 0.008*\"peopl\" + 0.008*\"industri\" + 0.008*\"make\" + 0.007*\"profit\" + 0.007*\"good\"'),\n",
       " (5,\n",
       "  '0.033*\"govern\" + 0.012*\"state\" + 0.009*\"peopl\" + 0.008*\"countri\" + 0.008*\"problem\" + 0.007*\"regul\" + 0.007*\"privat\" + 0.007*\"power\" + 0.007*\"obama\" + 0.007*\"public\"'),\n",
       " (6,\n",
       "  '0.024*\"market\" + 0.022*\"buy\" + 0.022*\"sell\" + 0.017*\"compani\" + 0.015*\"make\" + 0.014*\"price\" + 0.012*\"invest\" + 0.011*\"stock\" + 0.011*\"drug\" + 0.009*\"money\"'),\n",
       " (7,\n",
       "  '0.013*\"food\" + 0.010*\"babi\" + 0.009*\"make\" + 0.009*\"grandma\" + 0.009*\"eat\" + 0.008*\"don\" + 0.008*\"thing\" + 0.008*\"good\" + 0.007*\"grandmoth\" + 0.007*\"peopl\"'),\n",
       " (8,\n",
       "  '0.016*\"don\" + 0.014*\"time\" + 0.014*\"year\" + 0.013*\"peopl\" + 0.012*\"famili\" + 0.012*\"work\" + 0.010*\"make\" + 0.009*\"mom\" + 0.009*\"thing\" + 0.009*\"parent\"'),\n",
       " (9,\n",
       "  '0.012*\"don\" + 0.010*\"law\" + 0.009*\"talk\" + 0.008*\"point\" + 0.007*\"make\" + 0.007*\"understand\" + 0.007*\"peopl\" + 0.006*\"person\" + 0.006*\"case\" + 0.006*\"wrong\"')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.show_topics()\n",
    "# lda.print_topics()\n",
    "# lda.get_topic_terms(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0a30ac6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'corp1_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/hc/nb68sdy94m18z5wflv2b1v2c0000gn/T/ipykernel_1309/282653410.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# for doc in corp1_list:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorp1_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdoc_bow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcommon_dictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_document_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_bow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminimum_probability\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'corp1_list' is not defined"
     ]
    }
   ],
   "source": [
    "# for doc in corp1_list:\n",
    "doc = corp1_list[0]\n",
    "doc_bow = common_dictionary.doc2bow(doc)\n",
    "lda.get_document_topics(doc_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f1bd4d9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = {'a': 1, 'b': 2}\n",
    "'a' in a.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69609064",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
